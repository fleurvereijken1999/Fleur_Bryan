{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Data"
      ],
      "metadata": {
        "id": "113WoohC5oYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the datafiles from github"
      ],
      "metadata": {
        "id": "w1D1J5CHqnr2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj4xg8nw3duE",
        "outputId": "4128aa1e-edc0-485c-a05b-5ea2fd97c41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dl_bif_project_data'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Total 22 (delta 0), reused 0 (delta 0), pack-reused 22\n",
            "Unpacking objects: 100% (22/22), 8.74 MiB | 6.03 MiB/s, done.\n",
            "['len200_500_n5000nr3.pos', 'GO_3A0043066.annotprot', 'len200_500_n1000.pos', 'len100_200_n1000.pos', 'len200_500_n5000nr3.seq', 'len200_500_n5000nr2.pos', 'len100_200_n1000.seq', 'GO_3A0055085.annotprot', 'GO_3A0007165.annotprot', 'GO_3A0005576.annotprot', 'len200_500_n5000nr4.seq', 'len200_500_n5000nr2.seq', 'len200_500_n5000nr1.seq', 'len200_500_n5000nr1.pos', 'expr5Tseq_filtGO_100-1000.lis', 'GO_3A0005739.annotprot', 'test_set_filt.f', 'len200_500_n5000nr4.pos', 'len200_500_n1000.seq']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "! git clone https://git.wur.nl/dijk097/dl_bif_project_data.git\n",
        "os.chdir('dl_bif_project_data')\n",
        "print(os.listdir(\"project_datasets\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing d2l"
      ],
      "metadata": {
        "id": "BkImKO-Eqtru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==0.16 --quiet"
      ],
      "metadata": {
        "id": "tOdjZQvVR0je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required modules"
      ],
      "metadata": {
        "id": "Ok3ma5fCqyuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from d2l import torch as d2l\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "U0k7RhKCQ_us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D-CNN on the simulated datasets"
      ],
      "metadata": {
        "id": "Y0XWSmzonHpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read(seqfile,posfile):\n",
        "  '''Parsing two files in text format.\n",
        "\n",
        "  Keyword arguments:\n",
        "  seqfile -- file with sequences\n",
        "  posfile -- file with positive cases (annotated with function)\n",
        "  Returns:\n",
        "  datalist -- list containing all sequences\n",
        "  labellist -- list containing labels in binary format (0 = negative, \n",
        "  1 = positive)\n",
        "  '''\n",
        "  datalist = []\n",
        "  with open(seqfile, 'r') as seq:\n",
        "    count = 0\n",
        "    for line in seq:\n",
        "      line_spl = line.strip().split(\"\\t\")\n",
        "      datalist.append(line_spl[1])\n",
        "      count += 1\n",
        "    labellist = [0] * count #create list with zeros\n",
        "  with open(posfile, 'r') as pos:\n",
        "    for line in pos:\n",
        "      pos_str = line.strip()\n",
        "      pos_ind = pos_str[3:]\n",
        "      pos_num = int(pos_ind)\n",
        "      labellist[pos_num - 1] = 1\n",
        "  return datalist, labellist\n",
        "\n",
        "def read_halfpos(seqfile,posfile):\n",
        "  '''Parsing two files in text format. Reduce positives by 50%!\n",
        "\n",
        "  Keyword arguments:\n",
        "  seqfile -- file with sequences\n",
        "  posfile -- file with positive cases (annotated with function)\n",
        "  Returns:\n",
        "  datalist -- list containing all sequences\n",
        "  labellist -- list containing labels in binary format (0 = negative, \n",
        "  1 = positive) !50% less positives.\n",
        "  '''\n",
        "  datalist = []\n",
        "  pos_lst = []\n",
        "  with open(seqfile, 'r') as seq:\n",
        "    count = 0\n",
        "    for line in seq:\n",
        "      line_spl = line.strip().split(\"\\t\")\n",
        "      datalist.append(line_spl[1])\n",
        "      count += 1\n",
        "    labellist = [0] * count #create list with zeros\n",
        "  with open(posfile, 'r') as pos:\n",
        "    for line in pos:\n",
        "      pos_lst.append(line.strip())\n",
        "    half_pos = pos_lst[1::2] #exclude 50% of the positives\n",
        "    for item in half_pos:\n",
        "      pos_ind = item[3:]\n",
        "      pos_num = int(pos_ind)\n",
        "      labellist[pos_num - 1] = 1\n",
        "  return datalist, labellist\n",
        "\n",
        "def generate_train_test(datalist, labellist):\n",
        "  '''Split data into a training set (70%) and test set (30%). \n",
        "\n",
        "  Keyword arguments:\n",
        "  datalist -- list containing all sequences obtained from the read function\n",
        "  labellist -- list containing labels in binary format obtained from the \n",
        "  read function\n",
        "  Returns:\n",
        "  traindatalist -- list containing 70% of the data for the trainset\n",
        "  trainlabellist -- list containing the pos labels for the trainset\n",
        "  testdatalist -- list containing 30% of the data for testset\n",
        "  testlabellist -- list containing the pos labels for the testset\n",
        "  ! Data and labels are put into a tuple.\n",
        "  '''\n",
        "  length_seq = len(datalist)\n",
        "  len_70 = round(length_seq * 0.7)\n",
        "  traindatalist = []\n",
        "  trainlabellist = []\n",
        "  testdatalist = []\n",
        "  testlabellist= []\n",
        "  for count, seq in enumerate(datalist):\n",
        "    if count < len_70:\n",
        "      traindatalist.append(seq)\n",
        "    else:\n",
        "      testdatalist.append(seq)\n",
        "  for count, pos in enumerate(labellist):\n",
        "    if count < len_70:\n",
        "      trainlabellist.append(pos)\n",
        "    else:\n",
        "      testlabellist.append(pos)\n",
        "  return (traindatalist, trainlabellist), (testdatalist, testlabellist)\n",
        "\n",
        "def load_data(batch_size, num_steps, dataset):\n",
        "    '''Load data, tokenize and convert to one-hot encoding.\n",
        "\n",
        "    Keyword arguments:\n",
        "    batch_size -- the batch size for data loading\n",
        "    num_steps -- the number of steps for each sequence\n",
        "    dataset -- the dataset containing sequences and labels obtained \n",
        "    from the generate_train_test function.\n",
        "    Returns:\n",
        "    data_iter -- data iterator with batched one-hot encoded sequences\n",
        "    '''\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq, lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    # Convert to one-hot encoding\n",
        "    one_hot_seq_array = F.one_hot(seq_array, num_classes=21).float()\n",
        "    # Transpose the dimensions\n",
        "    one_hot_seq_array = one_hot_seq_array.transpose(1, 2)\n",
        "    # Add extra dimension to the target tensor and change data type to float\n",
        "    data_arrays = (one_hot_seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
        "    return data_iter\n",
        "\n",
        "def tokenize(dat, map2num,non_aa_num=20):\n",
        "  '''Tokenize the input data by mapping amino acids to numbers.\n",
        "\n",
        "  Keyword arguments:\n",
        "  dat -- input data as list of sequences\n",
        "  map2num -- dictionary mapping amino acids to numbers\n",
        "  non_aa_num -- number to replace non-amino acid characters (default: 20)\n",
        "  Returns:\n",
        "  seq -- list of tokenized sequences\n",
        "  '''\n",
        "  seq = []\n",
        "  for count, i in enumerate(dat):\n",
        "    seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "  return seq\n",
        "\n",
        "def build_seq_array(lines, num_steps,non_aa_num=20):\n",
        "  '''Build a sequence array from the input data.\n",
        "\n",
        "  Keyword arguments:\n",
        "  lines -- input data as list of sequences\n",
        "  num_steps -- the number of steps for each sequence\n",
        "  non_aa_num -- number to replace non-amino acid characters (default: 20)\n",
        "  Returns:\n",
        "  array -- tensor containing the padded or truncated sequences\n",
        "  '''\n",
        "  array = torch.tensor([\n",
        "  truncate_pad(l, num_steps, non_aa_num) for l in lines])\n",
        "  return array\n",
        "\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "  '''Truncate or pad a sequence based on the specified number of steps.\n",
        "\n",
        "  Keyword arguments:\n",
        "  line -- input sequence as a list of tokens\n",
        "  num_steps -- the number of steps for each sequence\n",
        "  padding_token -- token to pad sequences shorter than num_steps\n",
        "  Returns:\n",
        "  truncated or padded sequence\n",
        "  '''\n",
        "  if len(line) > num_steps:\n",
        "    return line[:num_steps] # Truncate the sequence if needed\n",
        "  return line + [padding_token] * (num_steps - len(line)) # Padding\n",
        "\n",
        "\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, momentum,\n",
        "              device=d2l.try_gpu()):\n",
        "    '''Train the 1D-CNN model.\n",
        "\n",
        "    Keyword arguments:\n",
        "    net -- model to train\n",
        "    train_iter -- training data iterator\n",
        "    test_iter -- test data iterator\n",
        "    num_epochs -- number of training epochs\n",
        "    lr -- learning rate\n",
        "    momentum -- momentum for the optimizer\n",
        "    device -- device to run the training on (default: GPU if available)\n",
        "    '''\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "    #optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "              animator.add(epoch + (i + 1) / num_batches,\n",
        "              (train_l, train_acc, None))\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "          f'test acc {test_acc:.3f}')\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "          f'on {str(device)}')\n",
        "\n",
        "\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None):  \n",
        "    '''Evaluate the accuracy of the model using a GPU.\n",
        "\n",
        "    Keyword arguments:\n",
        "    net -- model to evaluate\n",
        "    data_iter -- data iterator for the dataset to evaluate on\n",
        "    device -- device to run the evaluation on (default: None)\n",
        "    Returns:\n",
        "    accuracy -- proportion of correct predictions\n",
        "    '''\n",
        "    if isinstance(net, torch.nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # No. of correct predictions, no. of predictions\n",
        "    metric = d2l.Accumulator(2)\n",
        "    for X, y in data_iter:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "#Parsing files\n",
        "'''\n",
        "datalist, labellist = read(\"project_datasets/len100_200_n1000.seq\",\n",
        "                           \"project_datasets/len100_200_n1000.pos\"\n",
        "'''\n",
        "datalist, labellist = read(\"project_datasets/len200_500_n1000.seq\",\n",
        "                           \"project_datasets/len200_500_n1000.pos\")\n",
        "'''\n",
        "datalist, labellist = read(\"project_datasets/len200_500_n5000nr1.seq\",\n",
        "                           \"project_datasets/len200_500_n5000nr1.pos\")\n",
        "datalist, labellist = read(\"project_datasets/len200_500_n5000nr2.seq\",\n",
        "                           \"project_datasets/len200_500_n5000nr2.pos\")\n",
        "datalist, labellist = read(\"project_datasets/len200_500_n5000nr3.seq\",\n",
        "                           \"project_datasets/len200_500_n5000nr3.pos\")\n",
        "datalist, labellist = read(\"project_datasets/len200_500_n5000nr4.seq\",\n",
        "                           \"project_datasets/len200_500_n5000nr4.pos\")\n",
        "'''\n",
        "\n",
        "#Parsing files and decreasing positives by 50%\n",
        "'''\n",
        "datalist, labellist = read_halfpos(\"project_datasets/len100_200_n1000.seq\",\n",
        "                                   \"project_datasets/len100_200_n1000.pos\")\n",
        "'''\n",
        "\n",
        "#splitting datset into trainset and testset\n",
        "traindataset,testdataset =  generate_train_test(datalist, labellist)\n",
        "\n",
        "\n",
        "#Set batch size and number of steps (maximum sequence length) \n",
        "#change num_steps when using the len 100_200\n",
        "train_iter=load_data(20,200,traindataset) \n",
        "test_iter=load_data(20,200,testdataset)\n",
        "\n",
        "#The 1D-CNN network architecture\n",
        "net = nn.Sequential(\n",
        "    # Convolutional layer\n",
        "    nn.Conv1d(in_channels=21, out_channels=32, kernel_size=9, stride=1, padding=4),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # Max pooling layer\n",
        "    nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "    # Flatten layer\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # First fully connected layer, outputs 120\n",
        "    nn.Linear(32 * 100, 120),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # First dropout layer\n",
        "    nn.Dropout(0.8),\n",
        "\n",
        "    # Second fully connected layer, outputs 84\n",
        "    nn.Linear(120, 84),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # Second dropout layer\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    # Final fully connected layer, outputs 2 (for two classes)\n",
        "    nn.Linear(84, 2),\n",
        ")\n",
        "\n",
        "#Training the model\n",
        "train_ch6(net, train_iter, test_iter, num_epochs=40, lr=0.01, momentum = 0.9)"
      ],
      "metadata": {
        "id": "76XZo75HrLKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually look at the predicted output of one iteration"
      ],
      "metadata": {
        "id": "SxOfwvEU79GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(test_iter))\n",
        "device=d2l.try_gpu()\n",
        "X, y = x.to(device), y.to(device)\n",
        "y_hat = net(X)\n",
        "print(y_hat.argmax(axis=1))\n",
        "print(y)"
      ],
      "metadata": {
        "id": "UFhybRWNxr5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D-CNN on the biological dataset"
      ],
      "metadata": {
        "id": "gsRNEfWO8Qk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read(seqfile, *go_files):\n",
        "    '''Parse a sequence file and multiple Gene Ontology files.\n",
        "\n",
        "    Keyword arguments:\n",
        "    seqfile -- file containing sequences and labels\n",
        "    *go_files -- variable-length list of files containing Gene Ontology annotations\n",
        "    Returns:\n",
        "    datalist -- list containing all sequences\n",
        "    labellist -- list containing labels as integers (0 for not found,\n",
        "    1-n for corresponding GO files)\n",
        "    '''\n",
        "    datalist = []\n",
        "    labels = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as seq:\n",
        "        for line in seq:\n",
        "            line_spl = line.strip().split(\"\\t\")\n",
        "            datalist.append(line_spl[1])\n",
        "            labels.append(line_spl[0])\n",
        "    go_lists = []\n",
        "    for go_file in go_files:\n",
        "        go_list = []\n",
        "        with open(go_file, 'r') as f:\n",
        "            for line in f:\n",
        "                go_list.append(line.strip())\n",
        "        go_lists.append(go_list)\n",
        "    for item in range(len(labels)):\n",
        "        found = False\n",
        "        for i, go_list in enumerate(go_lists):\n",
        "            if labels[item] in go_list:\n",
        "                labellist.append(i + 1)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "          labellist.append(0)\n",
        "    return datalist, labellist\n",
        "\n",
        "def generate_train_test(datalist, labellist):\n",
        "  '''Split data into a training set (70%) and test set (30%). \n",
        "\n",
        "  Keyword arguments:\n",
        "  datalist -- list containing all sequences obtained from the read function\n",
        "  labellist -- list containing labels in binary format obtained from the \n",
        "  read function\n",
        "  Returns:\n",
        "  traindatalist -- list containing 70% of the data for the trainset\n",
        "  trainlabellist -- list containing the pos labels for the trainset\n",
        "  testdatalist -- list containing 30% of the data for testset\n",
        "  testlabellist -- list containing the pos labels for the testset\n",
        "  ! Data and labels are put into a tuple.\n",
        "  '''\n",
        "  length_seq = len(datalist)\n",
        "  len_70 = round(length_seq * 0.7)\n",
        "  traindatalist = []\n",
        "  trainlabellist = []\n",
        "  testdatalist = []\n",
        "  testlabellist= []\n",
        "  for count, seq in enumerate(datalist):\n",
        "    if count < len_70:\n",
        "      traindatalist.append(seq)\n",
        "    else:\n",
        "      testdatalist.append(seq)\n",
        "  for count, pos in enumerate(labellist):\n",
        "    if count < len_70:\n",
        "      trainlabellist.append(pos)\n",
        "    else:\n",
        "      testlabellist.append(pos)\n",
        "  return (traindatalist, trainlabellist), (testdatalist, testlabellist)\n",
        "\n",
        "def load_data(batch_size, num_steps, dataset):\n",
        "    '''Load data, tokenize and convert to one-hot encoding.\n",
        "\n",
        "    Keyword arguments:\n",
        "    batch_size -- the batch size for data loading\n",
        "    num_steps -- the number of steps for each sequence\n",
        "    dataset -- the dataset containing sequences and labels obtained \n",
        "    from the generate_train_test function.\n",
        "    Returns:\n",
        "    data_iter -- data iterator with batched one-hot encoded sequences\n",
        "    '''\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq, lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    # Convert to one-hot encoding\n",
        "    one_hot_seq_array = F.one_hot(seq_array, num_classes=21).float()\n",
        "    # Transpose the dimensions\n",
        "    one_hot_seq_array = one_hot_seq_array.transpose(1, 2)\n",
        "    # Add extra dimension to the target tensor and change data type to float\n",
        "    data_arrays = (one_hot_seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
        "    return data_iter\n",
        "\n",
        "def tokenize(dat, map2num,non_aa_num=20):\n",
        "  '''Tokenize the input data by mapping amino acids to numbers.\n",
        "\n",
        "  Keyword arguments:\n",
        "  dat -- input data as list of sequences\n",
        "  map2num -- dictionary mapping amino acids to numbers\n",
        "  non_aa_num -- number to replace non-amino acid characters (default: 20)\n",
        "  Returns:\n",
        "  seq -- list of tokenized sequences\n",
        "  '''\n",
        "  seq = []\n",
        "  for count, i in enumerate(dat):\n",
        "    seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "  return seq\n",
        "\n",
        "def build_seq_array(lines, num_steps,non_aa_num=20):\n",
        "  '''Build a sequence array from the input data.\n",
        "\n",
        "  Keyword arguments:\n",
        "  lines -- input data as list of sequences\n",
        "  num_steps -- the number of steps for each sequence\n",
        "  non_aa_num -- number to replace non-amino acid characters (default: 20)\n",
        "  Returns:\n",
        "  array -- tensor containing the padded or truncated sequences\n",
        "  '''\n",
        "  array = torch.tensor([\n",
        "  truncate_pad(l, num_steps, non_aa_num) for l in lines])\n",
        "  return array\n",
        "\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "  '''Truncate or pad a sequence based on the specified number of steps.\n",
        "\n",
        "  Keyword arguments:\n",
        "  line -- input sequence as a list of tokens\n",
        "  num_steps -- the number of steps for each sequence\n",
        "  padding_token -- token to pad sequences shorter than num_steps\n",
        "  Returns:\n",
        "  truncated or padded sequence\n",
        "  '''\n",
        "  if len(line) > num_steps:\n",
        "    return line[:num_steps] # Truncate the sequence if needed\n",
        "  return line + [padding_token] * (num_steps - len(line)) # Padding\n",
        "\n",
        "\n",
        "def train_ch6(net, train_iter, test_iter, num_epochs, lr, momentum,\n",
        "              device=d2l.try_gpu()):\n",
        "    '''Train the 1D-CNN model.\n",
        "\n",
        "    Keyword arguments:\n",
        "    net -- model to train\n",
        "    train_iter -- training data iterator\n",
        "    test_iter -- test data iterator\n",
        "    num_epochs -- number of training epochs\n",
        "    lr -- learning rate\n",
        "    momentum -- momentum for the optimizer\n",
        "    device -- device to run the training on (default: GPU if available)\n",
        "    '''\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv1d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "    net.apply(init_weights)\n",
        "    print('training on', device)\n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "    #optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'test acc'])\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train()\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "            train_l = metric[0] / metric[2]\n",
        "            train_acc = metric[1] / metric[2]\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (train_l, train_acc, None))\n",
        "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, test_acc))\n",
        "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "          f'test acc {test_acc:.3f}')\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "          f'on {str(device)}')\n",
        "\n",
        "\n",
        "def evaluate_accuracy_gpu(net, data_iter, device=None):  \n",
        "    '''Evaluate the accuracy of the model using a GPU.\n",
        "\n",
        "    Keyword arguments:\n",
        "    net -- model to evaluate\n",
        "    data_iter -- data iterator for the dataset to evaluate on\n",
        "    device -- device to run the evaluation on (default: None)\n",
        "    Returns:\n",
        "    accuracy -- proportion of correct predictions\n",
        "    '''\n",
        "    if isinstance(net, torch.nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # No. of correct predictions, no. of predictions\n",
        "    metric = d2l.Accumulator(2)\n",
        "    for X, y in data_iter:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "\n",
        "#Parsing files\n",
        "datalist, labellist = read(\"project_datasets/expr5Tseq_filtGO_100-1000.lis\",\n",
        "                           \"project_datasets/GO_3A0055085.annotprot\",\n",
        "                           \"project_datasets/GO_3A0043066.annotprot\",\n",
        "                           \"project_datasets/GO_3A0007165.annotprot\",\n",
        "                           \"project_datasets/GO_3A0005739.annotprot\",\n",
        "                           \"project_datasets/GO_3A0005576.annotprot\")\n",
        "\n",
        "#Calculate number of output classes in the network\n",
        "out_classes = len((\"project_datasets/GO_3A0055085.annotprot\",\n",
        "                   \"project_datasets/GO_3A0043066.annotprot\",\n",
        "                   \"project_datasets/GO_3A0007165.annotprot\",\n",
        "                   \"project_datasets/GO_3A0005739.annotprot\",\n",
        "                   \"project_datasets/GO_3A0005576.annotprot\")) + 1\n",
        "\n",
        "\n",
        "#splitting datset into trainset and testset\n",
        "traindataset,testdataset =  generate_train_test(datalist, labellist)\n",
        "\n",
        "\n",
        "#Set batch size and number of steps (maximum sequence length) \n",
        "train_iter=load_data(20,1000,traindataset) \n",
        "test_iter=load_data(20,1000,testdataset)\n",
        "\n",
        "#The 1D-CNN network architecture\n",
        "net = nn.Sequential(\n",
        "    # Convolutional layer\n",
        "    nn.Conv1d(in_channels=21, out_channels=32, kernel_size=9, stride=1, padding=4),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # Max pooling layer\n",
        "    nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "    # Flatten layer\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # First fully connected layer, outputs 120\n",
        "    nn.Linear(32 * 500, 120),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # First dropout layer\n",
        "    nn.Dropout(0.8),\n",
        "\n",
        "    # Second fully connected layer, outputs 84\n",
        "    nn.Linear(120, 84),\n",
        "    # ReLU activation function\n",
        "    nn.ReLU(),\n",
        "    # Second dropout layer\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    # Final fully connected layer, outputs=out classes (6 for used dataset)\n",
        "    nn.Linear(84, out_classes),\n",
        ")\n",
        "\n",
        "#Training the model\n",
        "train_ch6(net, train_iter, test_iter, num_epochs=40, lr=0.01, momentum = 0.9)"
      ],
      "metadata": {
        "id": "TwN4Ei6TCTX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually look at the predicted output of one iteration"
      ],
      "metadata": {
        "id": "gzjpYdlK8M4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(test_iter))\n",
        "device=d2l.try_gpu()\n",
        "X, y = x.to(device), y.to(device)\n",
        "y_hat = net(X)\n",
        "print(y_hat.argmax(axis=1))\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "aKkar2lRFR9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}